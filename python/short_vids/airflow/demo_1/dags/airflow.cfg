[core]
# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository
dags_folder = /usr/local/airflow/dags
load_examples = False

[webserver]
# The base url of your website as airflow cannot guess what domain or
# cname you are using. This is used in automated emails that
# airflow sends to point links to the right web server
base_url = http://localhost:8081

# The ip specified when starting the web server
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8081

[scheduler]
# Task instances listen for external kill signal (when you clear
# tasks) this is the frequency at which they should listen (in seconds).
job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks (look at the
# scheduler section in the docs for more information). This defines
# how often the scheduler should run (in seconds).
scheduler_heartbeat_sec = 5

[cli]
# In case you want to do a reset of the database, this is the command
# to use.
api_client = airflow
api_args = resetdb

[celery]
# This should contain a comma-separated list of queues to use
# for tasks
celery_queues = celery

# The app name that will be used by celery
celery_app_name = airflow.executors.celery_executor

# The concurrency that will be used when starting workers with the
# "airflow worker" command. This defines the number of task instances that
# a worker will take, so size up your workers based on the resources on
# your worker box and the nature of your tasks
worker_concurrency = 16

[sqlalchemy]
# The SqlAlchemy connection string to the metadata database.
# SqlAlchemy supports many different database engine, more information
# their website
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres:5432/airflow

# The SqlAlchemy pool size is the maximum number of database connections
# in the pool.
sql_alchemy_pool_size = 5

# The SqlAlchemy pool recycle is the number of seconds a connection
# can be idle in the pool before it is invalidated. This config does
# not apply to sqlite.
sql_alchemy_pool_recycle = 1800

[atlas]
sasl_enabled = True

[mesos]
# Mesos master address which MesosExecutor will connect to.
master = localhost:5050

# The framework name which Airflow scheduler will register itself as on mesos
framework_name = Airflow

# Number of cpu cores required for running one task instance using
# 'airflow run ...' command
task_cpu = 1

# Memory in MB required for running one task instance using
# 'airflow run ...' command
task_memory = 256

# Enable framework checkpointing for mesos
checkpoint = False

# Failover timeout in seconds.
failover_timeout = 604800

# Enable framework authentication for mesos
authenticate = False

# Mesos credentials, if authenticate is True
# mesos_authenticate_to_registry = False
# default_principal = admin
# default_secret = admin